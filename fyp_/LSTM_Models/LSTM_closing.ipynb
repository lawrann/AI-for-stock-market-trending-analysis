{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_closing_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj_R8sgq6hYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re2iY4-P6i6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from pandas import datetime\n",
        "import math, time\n",
        "import itertools\n",
        "from sklearn import preprocessing\n",
        "import datetime\n",
        "from operator import itemgetter\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import CuDNNLSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTmZLQvCZ_Z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_scores(filename):\n",
        "  print(filename)\n",
        "  lstm = load_model(filename)\n",
        "  predicted_stock_price = lstm.predict(X_test)\n",
        "  predicted_stock_price.shape\n",
        "  predicted_stock_price = sc.inverse_transform(predicted_stock_price) # denormalize values\n",
        "  trainScore = lstm.evaluate(X_train1, y_train1, verbose=0)\n",
        "  print('Train Score: %.20f MSE (%.20f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
        "  testScore = lstm.evaluate(X_test, y_test, verbose=0)\n",
        "  print('Test Score: %.20f MSE (%.20f RMSE)' % (testScore, math.sqrt(testScore)))\n",
        "  rsp = real_stock_price.iloc[:,0:1].values\n",
        "  predicted_stock_price = lstm.predict(X_test)\n",
        "  testMSE = mean_squared_error(y_test_actual, sc.inverse_transform(predicted_stock_price)[:,0])\n",
        "  testRMSE = np.sqrt(testMSE)\n",
        "  print(\"Test MSE:\",testMSE)\n",
        "  print(\"Test RMSE:\",testRMSE)\n",
        "\n",
        "  predicted_stock_price = lstm.predict(X_train1)\n",
        "  trainMSE = mean_squared_error(y_train_actual, sc.inverse_transform(predicted_stock_price)[:,0])\n",
        "  trainRMSE = np.sqrt(trainMSE)\n",
        "  print(\"Train MSE:\",trainMSE)\n",
        "  print(\"Train RMSE:\",trainRMSE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nsF0rjn6kS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNormalizeXyTrain(training_set_scaled, amount_of_features, seq_len):\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "  for i in range(seq_len,1007): # split 80:20\n",
        "      X_train.append(training_set_scaled[i-seq_len:i,0:amount_of_features])\n",
        "      y_train.append(training_set_scaled[i,0]) # predict the first row, open\n",
        "  X_train = np.array(X_train) # convert to np array\n",
        "  y_train = np.array(y_train) # convert to np array\n",
        "\n",
        "  print(X_train.shape) # no need reshape, already in 3d.\n",
        "  print(y_train.shape)\n",
        "  return X_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVdzjy3j6lND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_hidden_layer = '1'\n",
        "def build_LSTM(amount_of_features, seq_len, num_hidden_neuron, dropout_rate, ):\n",
        "  seed_value = 8\n",
        "  # Innitialise LSTM\n",
        "  layers = [amount_of_features,seq_len,1]\n",
        "  regressor = Sequential()\n",
        "\n",
        "  # LSTM 1\n",
        "  regressor.add(CuDNNLSTM(units=num_hidden_neuron, return_sequences=True, input_shape = (layers[1], layers[0]) ) )\n",
        "  regressor.add(Dropout(dropout_rate, seed = seed_value)) \n",
        "\n",
        "  #LSTM 2\n",
        "  regressor.add(CuDNNLSTM(units=num_hidden_neuron))\n",
        "  regressor.add(Dropout(dropout_rate, seed = seed_value))\n",
        "  \n",
        "  # output layer\n",
        "  opt = optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "  regressor.add(Dense(units=1, activation='relu'))\n",
        "  regressor.compile(optimizer=opt, loss='mean_squared_error')\n",
        "\n",
        "  return regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3YbzZlF6nQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STOCK_NAME = 'atvi'\n",
        "TRAIN_FILE_NAME = STOCK_NAME + '_train.csv'\n",
        "TEST_FILE_NAME =  STOCK_NAME + '_test.csv'\n",
        "TRAIN_FILE = r'/content/drive/My Drive/stock_data/' + TRAIN_FILE_NAME\n",
        "TEST_FILE = r'/content/drive/My Drive/stock_data/' + TEST_FILE_NAME\n",
        "STORE_FOLDER = r'/content/drive/My Drive/' + STOCK_NAME + '_plot/C/' \n",
        "\n",
        "path = '/content/drive/My Drive/' + STOCK_NAME + 'plot'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "path = '/content/drive/My Drive/' + STOCK_NAME + 'plot/C'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "seq_len = 3\n",
        "amount_of_features = 1\n",
        "num_of_epochs = 3000\n",
        "size_of_batch = 32\n",
        "p_val = 150\n",
        "num_hid_units = 512\n",
        "d_rate = 0.2\n",
        "\n",
        "# Load train set\n",
        "col_names = ['Date','Open','High','Low','Close','Adj Close','Volume','Tweet_Sentiment','Article_Sentiment']\n",
        "dataset_train = pd.read_csv(TRAIN_FILE, header=0, names=col_names) \n",
        "ds_train = dataset_train\n",
        "dataset_train = dataset_train.iloc[:1007,:].values # specify the number of rows\n",
        "training_set = pd.DataFrame(dataset_train)\n",
        "training_set.drop(training_set.columns[[0,1,2,3,5,6,7,8]],axis=1, inplace=True) # remove the columns u dont want\n",
        "training_set.columns = ['Close']\n",
        "\n",
        "#Normalize data\n",
        "sc = MinMaxScaler(feature_range=(0,1)) # feature range -> scale will be between 0 and 1\n",
        "training_set_scaled = sc.fit_transform(training_set) # normalize training_set\n",
        "\n",
        "# Load test set\n",
        "col_names = ['Date','Open','High','Low','Close','Adj Close','Volume','Tweet_Sentiment','Article_Sentiment']\n",
        "dataset_test = pd.read_csv(TEST_FILE, header=0, names=col_names)\n",
        "ds_test = dataset_test\n",
        "date = dataset_test.iloc[:251,0].values\n",
        "dataset_test = dataset_test.iloc[:251,:].values\n",
        "\n",
        "# realstockprice use for plotting later\n",
        "real_stock_price = pd.DataFrame(dataset_test)\n",
        "real_stock_price.drop(real_stock_price.columns[[0,1,2,3,5,6,7,8]],axis=1, inplace=True)\n",
        "real_stock_price.columns = ['Close']\n",
        "\n",
        "# getting the sequence length based test data\n",
        "dataset_total = pd.concat((ds_train[:1007], ds_test[:251]), axis = 0)\n",
        "dataset_total.drop(dataset_total.columns[[0,1,2,3,5,6,7,8]],axis=1, inplace=True) ## drop here when remove feature\n",
        "inputs = dataset_total[len(dataset_total)-len(ds_test) - seq_len:].values   # from the length of dataset test = 60 onwards\n",
        "\n",
        "# for getting real MSE value\n",
        "x_train_actual = []\n",
        "y_train_actual = []\n",
        "x_test_actual = [] \n",
        "y_test_actual = [] \n",
        "trainVals = training_set.values\n",
        "for i in range(seq_len,1007):\n",
        "    x_train_actual.append(trainVals[i-seq_len:i,0:amount_of_features])\n",
        "    y_train_actual.append(trainVals[i,0]) # predict the first row, open\n",
        "\n",
        "for i in range(seq_len, len(inputs)): # range depending on length of test data.\n",
        "    y_test_actual.append(inputs[i,0]) \n",
        "    x_test_actual.append(inputs[i-seq_len:i,0:amount_of_features])\n",
        "\n",
        "# normalize test inputs\n",
        "inputs = sc.transform(inputs)\n",
        "inputs.shape\n",
        "X_test = []\n",
        "y_test = []\n",
        "for i in range(seq_len, len(inputs)): # range depending on length of test data.\n",
        "    X_test.append(inputs[i-seq_len:i,0:amount_of_features]) # can try add sentiment effect %\n",
        "    y_test.append(inputs[i,0]) \n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-qu7WJy6q-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = '_'\n",
        "for i in range(len(real_stock_price.columns)):\n",
        "  feature = feature + str(real_stock_price.columns[i])\n",
        "file_name = (STORE_FOLDER +\n",
        "            STOCK_NAME +\n",
        "            '_e' + str(num_of_epochs) + \n",
        "            '_b' + str(size_of_batch) +\n",
        "            '_f' + str(feature) +\n",
        "            '_s' + str(seq_len) +\n",
        "            '_h' + str(num_hid_units) +\n",
        "            '_l' + str(num_hidden_layer) +\n",
        "            '_best_model.h5')\n",
        "X_train1, y_train1 = getNormalizeXyTrain(training_set_scaled, amount_of_features, seq_len)\n",
        "print(STOCK_NAME)\n",
        "print(feature)\n",
        "print(num_hid_units)\n",
        "print(num_hidden_layer)\n",
        "print(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvdJJdtrBxHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_list = []\n",
        "testscore_list = []\n",
        "testmse_list = []\n",
        "testRMSE_list = []\n",
        "\n",
        "finalTestScore_list = []\n",
        "finalTestMSE_list = []\n",
        "finalTestRMSE_list = []\n",
        "\n",
        "for i in range(10):\n",
        "  file_name = (STORE_FOLDER +\n",
        "            STOCK_NAME +\n",
        "            '_e' + str(num_of_epochs) + \n",
        "            '_b' + str(size_of_batch) +\n",
        "            '_f' + str(feature) +\n",
        "            '_s' + str(seq_len) +\n",
        "            '_h' + str(num_hid_units) +\n",
        "            '_l' + str(num_hidden_layer) +\n",
        "            '_averageten_' +\n",
        "            str(i) + \n",
        "            '_.h5')\n",
        "  print(file_name)\n",
        "\n",
        "  print(os.path.exists(file_name))\n",
        "  if not os.path.exists(file_name):\n",
        "    lstm = build_LSTM(amount_of_features, seq_len, num_hidden_neuron = num_hid_units, dropout_rate = d_rate)\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = p_val)\n",
        "    mc = ModelCheckpoint(file_name,  monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "    lstm.fit(X_train1, y_train1, validation_data = (X_test, y_test), epochs=num_of_epochs, batch_size=size_of_batch, callbacks=[es, mc], verbose=0)\n",
        "\n",
        "  lstm_list.append(load_model(file_name))\n",
        "\n",
        "  predicted_stock_price = lstm_list[i].predict(X_test)\n",
        "  predicted_stock_price = sc.inverse_transform(predicted_stock_price) # denormalize values\n",
        "  trainScore = lstm_list[i].evaluate(X_train1, y_train1, verbose=0)\n",
        "  print('Train Score: %.20f MSE (%.20f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
        "  testScore = lstm_list[i].evaluate(X_test, y_test, verbose=0)\n",
        "  print('Test Score: %.20f MSE (%.20f RMSE)' % (testScore, math.sqrt(testScore)))\n",
        "  rsp = real_stock_price.iloc[:,0:1].values\n",
        "  predicted_stock_price = lstm_list[i].predict(X_test)\n",
        "  testMSE = mean_squared_error(y_test_actual, sc.inverse_transform(predicted_stock_price)[:,0])\n",
        "  testRMSE = np.sqrt(testMSE)\n",
        "  print(\"Test MSE:\",testMSE)\n",
        "  print(\"Test RMSE:\",testRMSE)\n",
        "\n",
        "  predicted_stock_price = lstm_list[i].predict(X_train1)\n",
        "  trainMSE = mean_squared_error(y_train_actual, sc.inverse_transform(predicted_stock_price)[:,0])\n",
        "  trainRMSE = np.sqrt(trainMSE)\n",
        "  print(\"Train MSE:\",trainMSE)\n",
        "  print(\"Train RMSE:\",trainRMSE)\n",
        "\n",
        "  testscore_list.append(testScore)\n",
        "  testmse_list.append(testMSE)\n",
        "  testRMSE_list.append(testRMSE)\n",
        "\n",
        "t = 0\n",
        "for i in testscore_list:\n",
        "  t = t + i\n",
        "print(t/10)\n",
        "finalTestScore_list.append(t/10)\n",
        "t = 0\n",
        "for i in testmse_list:\n",
        "  t = t + i\n",
        "print(t/10)\n",
        "finalTestMSE_list.append(t/10)\n",
        "t = 0\n",
        "for i in testRMSE_list:\n",
        "  t = t + i\n",
        "print(t/10)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
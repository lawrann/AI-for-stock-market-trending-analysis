{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_closing_tweets_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca1gtByc8toj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8klI3a38xbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from pandas import datetime\n",
        "import math, time\n",
        "import itertools\n",
        "from sklearn import preprocessing\n",
        "import datetime\n",
        "from operator import itemgetter\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import CuDNNLSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhT9nxmqaBnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_scores(filename):\n",
        "  print(filename)\n",
        "  lstm = load_model(filename)\n",
        "  predicted_stock_price = lstm.predict(X_test)\n",
        "  predicted_stock_price.shape\n",
        "  predicted_stock_price = np.hstack((predicted_stock_price, np.zeros((predicted_stock_price.shape[0], 1), dtype=predicted_stock_price.dtype))) # depending on the number of features.\n",
        "  predicted_stock_price = sc.inverse_transform(predicted_stock_price) # denormalize values\n",
        "  trainScore = lstm.evaluate(X_train1, y_train1, verbose=0)\n",
        "  print('Train Score: %.20f MSE (%.20f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
        "  testScore = lstm.evaluate(X_test, y_test, verbose=0)\n",
        "  print('Test Score: %.20f MSE (%.20f RMSE)' % (testScore, math.sqrt(testScore)))\n",
        "  rsp = real_stock_price.iloc[:,0:1].values\n",
        "  predicted_stock_price = lstm.predict(X_test)\n",
        "  predicted_stock_price = np.hstack((predicted_stock_price, np.zeros((predicted_stock_price.shape[0], 1), dtype=predicted_stock_price.dtype))) # depending on the number of features.\n",
        "  testMSE = mean_squared_error(y_test_actual, sc.inverse_transform(predicted_stock_price)[:,0])\n",
        "  testRMSE = np.sqrt(testMSE)\n",
        "  print(\"Test MSE:\",testMSE)\n",
        "  print(\"Test RMSE:\",testRMSE)\n",
        "\n",
        "  predicted_stock_price = lstm.predict(X_train1)\n",
        "  predicted_stock_price = np.hstack((predicted_stock_price, np.zeros((predicted_stock_price.shape[0], 1), dtype=predicted_stock_price.dtype))) # depending on the number of features.\n",
        "  trainMSE = mean_squared_error(y_train_actual, sc.inverse_transform(predicted_stock_price)[:,0])\n",
        "  trainRMSE = np.sqrt(trainMSE)\n",
        "  print(\"Train MSE:\",trainMSE)\n",
        "  print(\"Train RMSE:\",trainRMSE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHQ-BHuP-iiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNormalizeXyTrain(training_set_scaled, amount_of_features, seq_len):\n",
        "  #sentiment_coefficient_matrix = getExponentialDecayMatrix(seq_len)\n",
        "  sentiment_coefficient_matrix = getSentimentMatrix(sentiment_coefficient,seq_len)\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "  for i in range(seq_len,1007):\n",
        "      x_t = np.copy(training_set_scaled[i-seq_len:i,0:amount_of_features])\n",
        "      X_train.append(x_t)\n",
        "      y_train.append(training_set_scaled[i,0]) # predict the first row, open\n",
        "\n",
        "  # applies to tweet sentiment\n",
        "  for X in X_train:\n",
        "    for index in range(len(sentiment_coefficient_matrix)):\n",
        "        value = X[index:index+1,1]\n",
        "        value = value*sentiment_coefficient_matrix[index]\n",
        "        X[index:index+1,1] = value\n",
        "        #X[index:index+1,1] = X[index:index+1,1] * sentiment_coefficient_matrix[index]\n",
        "\n",
        "  X_train = np.asarray(X_train)\n",
        "  y_train = np.asarray(y_train)\n",
        "  print(X_train.shape) # no need reshape, already in 3d.\n",
        "  print(y_train.shape)\n",
        "  return X_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMmLSXrEl0fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getSentimentMatrix(sentiment_coefficient,sequence_length):\n",
        "  sentiment_coefficient_matrix = []\n",
        "  # can explore e^ -time as the sentiment matrix\n",
        "  for i in range(sequence_length):\n",
        "    sentiment_coefficient_matrix.append(sentiment_coefficient**(sequence_length-(i+1)))\n",
        "  return sentiment_coefficient_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7OzFQNBiXOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def getExponentialDecayMatrix(sequence_length):\n",
        "  exponential_decay_matrix = []\n",
        "  for i in range(seq_len):\n",
        "    exponential_decay_matrix.append(math.exp(-(sequence_length-(i+1))))\n",
        "  return exponential_decay_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO5n0s76-jcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_hidden_layer = '1'\n",
        "def build_LSTM(amount_of_features, seq_len, num_hidden_neuron, dropout_rate, ):\n",
        "  seed_value = 8\n",
        "  # Innitialise LSTM\n",
        "  layers = [amount_of_features,seq_len,1]\n",
        "  regressor = Sequential()\n",
        "\n",
        "  #LSTM 1\n",
        "  regressor.add(CuDNNLSTM(units=num_hidden_neuron, return_sequences=True, input_shape = (layers[1], layers[0]) ) )\n",
        "  regressor.add(Dropout(dropout_rate, seed = seed_value)) \n",
        "  #LSTM 2\n",
        "  regressor.add(CuDNNLSTM(units=num_hidden_neuron))\n",
        "  regressor.add(Dropout(dropout_rate, seed = seed_value))\n",
        "  \n",
        "  # output layer\n",
        "  opt = optimizers.Adam(lr=0.001, decay=1e-6)\n",
        "  regressor.add(Dense(units=1, activation='relu'))\n",
        "  regressor.compile(optimizer=opt, loss='mean_squared_error')\n",
        "\n",
        "  return regressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag7I7bDj-lQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STOCK_NAME = 'spy'\n",
        "TRAIN_FILE_NAME = STOCK_NAME + '_train.csv'\n",
        "TEST_FILE_NAME =  STOCK_NAME + '_test.csv'\n",
        "TRAIN_FILE = r'/content/drive/My Drive/stock_data/' + TRAIN_FILE_NAME\n",
        "TEST_FILE = r'/content/drive/My Drive/stock_data/' + TEST_FILE_NAME\n",
        "STORE_FOLDER = r'/content/drive/My Drive/' + STOCK_NAME + '_plot/CT/' \n",
        "\n",
        "path = '/content/drive/My Drive/' + STOCK_NAME + '_plot'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "path = '/content/drive/My Drive/' + STOCK_NAME + '_plot/CT'\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "sentiment_coefficient = 0.075 # 0.001 (0.1%) 0.0001(0.01%) 0.00005(0.005%) 0.00001(0.001%)\n",
        "seq_len = 3\n",
        "amount_of_features = 2\n",
        "num_of_epochs = 3000\n",
        "size_of_batch = 32\n",
        "p_val = 150\n",
        "num_hid_units = 512\n",
        "d_rate = 0.2\n",
        "sentiment_coefficient_matrix = getSentimentMatrix(sentiment_coefficient,seq_len)\n",
        "\n",
        "# Load train set\n",
        "col_names = ['Date','Open','High','Low','Close','Adj Close','Volume','Tweet_Sentiment','Article_Sentiment']\n",
        "dataset_train = pd.read_csv(TRAIN_FILE, header=0, names=col_names) \n",
        "ds_train = dataset_train\n",
        "dataset_train = dataset_train.iloc[:1007,:].values # specify the number of rows\n",
        "training_set = pd.DataFrame(dataset_train)\n",
        "training_set.drop(training_set.columns[[0,1,2,3,5,6,8]],axis=1, inplace=True) # remove the columns u dont want\n",
        "training_set.columns = ['Close','Tweet_Sentiment']\n",
        "columnsTitles=['Close','Tweet_Sentiment'] # SWAP COLUMNS\n",
        "training_set = training_set.reindex(columns=columnsTitles) # SWAP COLUMNS\n",
        "#training_set.columns = ['Open','High','Low']\n",
        "\n",
        "#Normalize data\n",
        "sc = MinMaxScaler(feature_range=(0,1)) # feature range -> scale will be between 0 and 1\n",
        "norm_close = sc.fit_transform(training_set['Close'].to_frame()) # normalize closing\n",
        "norm_close = pd.DataFrame({'Close': norm_close[:, 0]})\n",
        "frames = [norm_close, training_set['Tweet_Sentiment'].to_frame()]\n",
        "training_set_scaled = pd.concat(frames, axis=1)\n",
        "training_set_scaled = training_set_scaled.to_numpy()\n",
        "\n",
        "# Load test set\n",
        "col_names = ['Date','Open','High','Low','Close','Adj Close','Volume','Tweet_Sentiment','Article_Sentiment']\n",
        "dataset_test = pd.read_csv(TEST_FILE, header=0, names=col_names)\n",
        "ds_test = dataset_test\n",
        "date = dataset_test.iloc[:251,0].values\n",
        "dataset_test = dataset_test.iloc[:251,:].values\n",
        "\n",
        "# realstockprice use for plotting later\n",
        "real_stock_price = pd.DataFrame(dataset_test)\n",
        "real_stock_price.drop(real_stock_price.columns[[0,1,2,3,5,6,8]],axis=1, inplace=True)\n",
        "real_stock_price.columns = ['Close','Tweet_Sentiment']\n",
        "real_stock_price = real_stock_price.reindex(columns=columnsTitles) # SWAP COLUMNS\n",
        "\n",
        "# getting the sequence length based test data\n",
        "dataset_total = pd.concat((ds_train[:1007], ds_test[:251]), axis = 0)\n",
        "dataset_total.drop(dataset_total.columns[[0,1,2,3,5,6,8]],axis=1, inplace=True) ## drop here when remove feature\n",
        "dataset_total = dataset_total.reindex(columns=columnsTitles) # SWAP COLUMNS\n",
        "inputs = dataset_total[len(dataset_total)-len(ds_test) - seq_len:].values   # from the length of dataset test = 60 onwards\n",
        "\n",
        "# for getting real MSE value\n",
        "x_train_actual = []\n",
        "y_train_actual = []\n",
        "x_test_actual = [] \n",
        "y_test_actual = [] \n",
        "trainVals = training_set.values\n",
        "for i in range(seq_len,1007):\n",
        "    x_train_actual.append(trainVals[i-seq_len:i,0:amount_of_features])\n",
        "    y_train_actual.append(trainVals[i,0]) # predict the first row, open\n",
        "\n",
        "for i in range(seq_len, len(inputs)): # range depending on length of test data.\n",
        "    y_test_actual.append(inputs[i,0]) \n",
        "    x_test_actual.append(inputs[i-seq_len:i,0:amount_of_features])\n",
        "\n",
        "# normalize test inputs\n",
        "norm_input_close = sc.transform(inputs[:,0:1])\n",
        "norm_input_close = pd.DataFrame({'Close': norm_input_close[:,0]})\n",
        "norm_input_tweet = pd.DataFrame({'Tweet_Sentiment': inputs[:,1]})\n",
        "frames = [norm_input_close, norm_input_tweet]\n",
        "inputs = pd.concat(frames, axis=1)\n",
        "inputs = inputs.to_numpy()\n",
        "X_test = []\n",
        "y_test = []\n",
        "for i in range(seq_len, len(inputs)): # range depending on length of test data.\n",
        "    X_test.append(inputs[i-seq_len:i,0:amount_of_features])\n",
        "    y_test.append(inputs[i,0]) \n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM07ZcxFANKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = '_'\n",
        "for i in range(len(real_stock_price.columns)):\n",
        "  feature = feature + str(real_stock_price.columns[i])\n",
        "file_name = (STORE_FOLDER +\n",
        "            STOCK_NAME +\n",
        "            '_e' + str(num_of_epochs) + \n",
        "            '_b' + str(size_of_batch) +\n",
        "            '_f' + str(feature) +\n",
        "            '_s' + str(seq_len) +\n",
        "            '_h' + str(num_hid_units) +\n",
        "            '_l' + str(num_hidden_layer) +\n",
        "            '_sc' + str(sentiment_coefficient) +\n",
        "            '_best_model.h5')\n",
        "X_train1, y_train1 = getNormalizeXyTrain(training_set_scaled, amount_of_features, seq_len)\n",
        "print(STOCK_NAME)\n",
        "print(feature)\n",
        "print(num_hid_units)\n",
        "print(num_hidden_layer)\n",
        "print(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3Nk2yf3gYdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exponential_flag = False # remember to change func used in getnormalize\n",
        "\n",
        "lstm_list = []\n",
        "testscore_list = []\n",
        "testmse_list = []\n",
        "testRMSE_list = []\n",
        "finalTestScore_list = []\n",
        "finalTestMSE_list = []\n",
        "finalTestRMSE_list = []\n",
        "\n",
        "for i in range(10): \n",
        "  if exponential_flag :\n",
        "    file_name = (STORE_FOLDER +\n",
        "                STOCK_NAME +\n",
        "                '_e' + str(num_of_epochs) + \n",
        "                '_b' + str(size_of_batch) +\n",
        "                '_f' + str(feature) +\n",
        "                '_s' + str(seq_len) +\n",
        "                '_h' + str(num_hid_units) +\n",
        "                '_l' + str(num_hidden_layer) +\n",
        "                '_scExponential'  +\n",
        "                '_averageten_' +\n",
        "                str(i) + \n",
        "                '_.h5')\n",
        "  else:\n",
        "    file_name = (STORE_FOLDER +\n",
        "                STOCK_NAME +\n",
        "                '_e' + str(num_of_epochs) + \n",
        "                '_b' + str(size_of_batch) +\n",
        "                '_f' + str(feature) +\n",
        "                '_s' + str(seq_len) +\n",
        "                '_h' + str(num_hid_units) +\n",
        "                '_l' + str(num_hidden_layer) +\n",
        "                '_sc' + str(sentiment_coefficient) +\n",
        "                '_averageten_' +\n",
        "                str(i) + \n",
        "                '_.h5')\n",
        "  print(file_name)\n",
        "\n",
        "  if not os.path.exists(file_name):\n",
        "    lstm = build_LSTM(amount_of_features, seq_len, num_hidden_neuron = num_hid_units, dropout_rate = d_rate)\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = p_val)\n",
        "    mc = ModelCheckpoint(file_name,  monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "    lstm.fit(X_train1, y_train1, validation_data = (X_test, y_test), epochs=num_of_epochs, batch_size=size_of_batch, callbacks=[es, mc], verbose=0)\n",
        "  \n",
        "  lstm_list.append(load_model(file_name))\n",
        "  predicted_stock_price = lstm_list[i].predict(X_test)\n",
        "  predicted_stock_price = sc.inverse_transform(predicted_stock_price) # denormalize values\n",
        "  trainScore = lstm_list[i].evaluate(X_train1, y_train1, verbose=0)\n",
        "  print('Train Score: %.20f MSE (%.20f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
        "  testScore = lstm_list[i].evaluate(X_test, y_test, verbose=0)\n",
        "  print('Test Score: %.20f MSE (%.20f RMSE)' % (testScore, math.sqrt(testScore)))\n",
        "  rsp = real_stock_price.iloc[:,0:1].values\n",
        "  predicted_stock_price = lstm_list[i].predict(X_test)\n",
        "  testMSE = mean_squared_error(y_test_actual, sc.inverse_transform(predicted_stock_price)[:,0])\n",
        "  testRMSE = np.sqrt(testMSE)\n",
        "  print(\"Test MSE:\",testMSE)\n",
        "  print(\"Test RMSE:\",testRMSE)\n",
        "\n",
        "  predicted_stock_price = lstm_list[i].predict(X_train1)\n",
        "  trainMSE = mean_squared_error(y_train_actual, sc.inverse_transform(predicted_stock_price)[:,0])\n",
        "  trainRMSE = np.sqrt(trainMSE)\n",
        "  print(\"Train MSE:\",trainMSE)\n",
        "  print(\"Train RMSE:\",trainRMSE)\n",
        "\n",
        "  testscore_list.append(testScore)\n",
        "  testmse_list.append(testMSE)\n",
        "  testRMSE_list.append(testRMSE)\n",
        "\n",
        "t = 0\n",
        "for i in testscore_list:\n",
        "  t = t + i\n",
        "print(t/10)\n",
        "t = 0\n",
        "for i in testmse_list:\n",
        "  t = t + i\n",
        "print(t/10)\n",
        "t = 0\n",
        "for i in testRMSE_list:\n",
        "  t = t + i\n",
        "print(t/10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}